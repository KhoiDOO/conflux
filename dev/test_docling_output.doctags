<doctag><picture><loc_54><loc_3><loc_90><loc_22></picture>
<text><loc_121><loc_1><loc_404><loc_23>This ICCV paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore.</text>
<section_header_level_1><loc_99><loc_65><loc_420><loc_77>VGGSound er : Audio-Visual Evaluations for Foundation Models</section_header_level_1>
<text><loc_67><loc_91><loc_433><loc_136>Daniil Zverev 1, * Thadd¨ aus Wiedemer 2,3,4,* Ameya Prabhu 2,3 Matthias Bethge 2,3 Wieland Brendel 3,4 A. Sophia Koepke 1,2,3 1 Technical University of Munich, MCML 2 University of T¨ ubingen 3 T¨ ubingen AI Center 4 MPI for Intelligent Systems, ELLIS Institute T¨ ubingen</text>
<text><loc_54><loc_140><loc_443><loc_151>{ daniil.zverev,a-sophia.koepke } @tum.de , { thaddaeus.wiedemer,ameya.prabhu } @uni-tuebingen.de</text>
<section_header_level_1><loc_126><loc_166><loc_162><loc_173>Abstract</section_header_level_1>
<text><loc_48><loc_183><loc_241><loc_317>The emergence of audio-visual foundation models underscores the importance of reliably assessing their multimodal understanding. The VGGSound dataset is commonly used as a benchmark for evaluation audio-visual classification. However, our analysis identifies several limitations of VGGSound, including incomplete labelling, partially overlapping classes, and misaligned modalities. These lead to distorted evaluations of auditory and visual capabilities. To address these limitations, we introduce VGGSounder, a comprehensively re-annotated, multi-label test set that extends VGGSound and is specifically designed to evaluate audio-visual foundation models. VGGSounder features detailed modality annotations, enabling precise analyses of modality-specific performance. Furthermore, we reveal model limitations by analysing performance degradation when adding another input modality with our new modality confusion metric. Our dataset and project page are available at https://vggsounder.github.io/ .</text>
<section_header_level_1><loc_48><loc_334><loc_111><loc_342>1. Introduction</section_header_level_1>
<text><loc_48><loc_348><loc_241><loc_437>Rigorous evaluation benchmarks have been instrumental in assessing the effectiveness of audio-visual models [33, 43, 49, 57]. Specifically, multi-modal foundation models integrating visual and auditory data aim to achieve a holistic understanding of audio-visual content. However, the field lacks large-scale modality-aware classification benchmarks with ground-truth annotations indicating whether each label is visible, audible, or both. Such annotations would allow detailed evaluations of multi-modal model capabilities. To address this gap, we introduce VGGSound er , an enhanced version of the widely-used audio-visual classification dataset VGGSound [13], which facilitates modality-</text>
<footnote><loc_57><loc_445><loc_107><loc_450>* equal contribution</footnote>
<picture><loc_260><loc_165><loc_451><loc_284><caption><loc_259><loc_292><loc_452><loc_347>Figure 1. We introduce VGGSound er , a multi-label audiovisual classification benchmark with modality annotations . We extend the original VGGSound test set with human-annotated audible , visible , and visible + audible labels. We add meta labels for common confounders, such as background music. We benchmark eleven recent audio-visual models on VGGSound er . It enables selective analysis of a model's auditory and visual capabilities on classes relevant for the queried modality.</caption></picture>
<text><loc_259><loc_360><loc_430><loc_366>aware evaluation of audio-visual foundation models.</text>
<text><loc_259><loc_369><loc_452><loc_450>VGGSound has several notable limitations. First, its data is inherently multi-label; for instance, a single sample may simultaneously include labels such as playing drum kit and playing acoustic guitar when multiple instruments are present. Additionally, evaluating how different modalities contribute to model performance becomes difficult without explicit modality annotations, as some labels are either not visually present or not audible (e.g., certain instruments might only be audible but not visible in advertisements). Moreover, overlapping label classes present another challenge; for example,</text>
<page_footer><loc_243><loc_472><loc_257><loc_480>1027</page_footer>
<page_break>
<text><loc_48><loc_47><loc_241><loc_68>the orchestra label often coincides with labels for individual instruments. These issues result in systematic underevaluation of multi-modal audio-visual foundation models.</text>
<text><loc_48><loc_70><loc_241><loc_257>To overcome these limitations, we present VGGSound er , an improved benchmark inspired by similar advancements in other domains [12, 28]. We re-annotate the dataset to create a comprehensive multi-label classification setting by collecting detailed annotations for each sample, including (1) additional classes present, (2) explicit modality annotations to label modality misalignment, (3) metadata indicating the presence of background music, voice-over, or static images, and (4) merging of classes to address overlapping classes. Consequently, VGGSound er provides a robust, foundation-model-ready benchmark enabling structured analysis of whether models rely on audio or visual cues. Furthermore, we include meta-labels (e.g., background music, voice-over, or static images) to easily filter out unreliable labels during evaluation. Utilising VGGSound er , we evaluate audio-visual foundation models, demonstrating their poor performance on our benchmark. We find that the state-of-the-art, closed-source Gemini models consistently rely exclusively on the visual modality. In addition to that, we measure the modality confusion, i.e. when models get distracted by an additional input modality, which exposes the unsuccessful merging of modalities. These findings highlight the importance of the audio-focused VGGSound er benchmark as a critical tool for accurately assessing audio-visual foundation models.</text>
<text><loc_53><loc_268><loc_176><loc_274>We make the following contributions:</text>
<ordered_list><list_item><loc_53><loc_276><loc_223><loc_282>We illustrate limitations of VGGSound in Sec. 3.</list_item>
<list_item><loc_53><loc_283><loc_236><loc_297>Wecurate VGGSound er with multi-modal human annotations for multi-label classification in Sec. 4.</list_item>
<list_item><loc_53><loc_298><loc_236><loc_319>We evaluate state-of-the-art audio-visual models, observing differences between embedding models and autoregressive foundation models in Sec. 5.</list_item>
<list_item><loc_53><loc_321><loc_236><loc_334>We propose new metrics to quantify the negative impact of using multiple input modalities in Sec. 5.</list_item>
</ordered_list>
<section_header_level_1><loc_48><loc_346><loc_114><loc_354>2. Related work</section_header_level_1>
<text><loc_48><loc_361><loc_241><loc_443>Audio-visual learning Many prior works consider audiovisual tasks that include sound source localisation and separation [3, 5, 9, 15, 27, 56, 62, 67, 75, 80, 85, 86, 90], event localisation [50, 51, 74, 78], audio-visual question answering [48, 54, 83, 84], audio-visual synchronisation [14, 23, 25, 38, 39, 42], audio synthesis using visual information [19, 26, 31, 44, 45, 61, 69-71, 87], or audio-driven face image synthesis [7, 40, 77]. Audio-visual data has also been leveraged for speech-related tasks, including speech and speaker recognition [2, 4, 59], or the spotting of spoken keywords [58, 66].</text>
<text><loc_48><loc_444><loc_241><loc_450>Furthermore, the natural alignment between audio and</text>
<otsl><loc_260><loc_45><loc_452><loc_99><ched>Dataset<ched># Clips<ched># Classes<ched>Multi- label<ched>Modality labels<ched>Annotation pipeline<nl><rhed>Flickr-SoundNet [10]<fcel>2M<fcel>-<fcel>✗<fcel>✗<fcel>-<nl><rhed>Kinetics-Sound [7]<fcel>18.8k<fcel>34<fcel>✗<fcel>✗<fcel>MTurk<nl><rhed>AudioSet [26]<fcel>2.1M<fcel>537<fcel>✓<fcel>✗<fcel>Manual<nl><rhed>AVE [62]<fcel>4k<fcel>28<fcel>✓<fcel>✗<fcel>Manual<nl><rhed>VEGAS [76]<fcel>132k<fcel>10<fcel>✗<fcel>✗<fcel>MTurk<nl><rhed>Visually Aligned Sounds [15]<fcel>13k<fcel>8<fcel>✗<fcel>✗<fcel>MTurk<nl><rhed>VGGSound [12]<fcel>200k<fcel>309<fcel>✗<fcel>✗<fcel>Classifiers+Manual<nl><rhed>VGGSound-Sparse [32]<fcel>7.1k<fcel>12<fcel>✗<fcel>✗<fcel>Manual<nl><rhed>Visual Sound [66]<fcel>91k<fcel>309<fcel>✗<fcel>✗<fcel>ImageBind [30]<nl><rhed>VGGSound er<fcel>15.4k<fcel>309<fcel>✓<fcel>✓<fcel>MTurk<nl><caption><loc_262><loc_106><loc_449><loc_111>Table 1. Comparison of audio-visual classification benchmarks.</caption></otsl>
<text><loc_259><loc_123><loc_452><loc_196>video has been exploited to learn improved audio-visual embeddings for downstream tasks [6, 10, 11, 18, 20, 21, 46, 60, 63-65, 79]. Using both modalities jointly generally leads to performance boosts over using one modality in isolation. We examine this observation closely and aim to evaluate the effective use of multiple input modalities for the video classification task. To enable this, we propose to the best of our knowledge, the first multi-label video classification benchmark that includes per-modality annotations for every sample (see Tab. 1).</text>
<text><loc_259><loc_200><loc_452><loc_350>Audio-visual foundation models Recently, multi-modal general-purpose models have emerged that can handle diverse downstream tasks without task-specific finetuning also referred to as multi-modal foundation models. For instance, images or language were used as the bridge between modalities including audio, image, and text [30, 89]. Building on this, PandaGPT [72] leverages Vicuna [22] and ImageBind's embedding space to train a general multi-modal model exclusively on image-text pairs. Unified-IO 2 [53] employs universal tokenisation to process audio, video, and text. VideoLLaMA2 [21] uses a Spatial-Temporal Convolution connector in the visual branch before projecting audio and visual information into the LLM input space. The recently introduced Ola model [52] advances omnimodal processing through progressive modality alignment, using video to bridge audio and visual information. The Gemini models [73] are closed-source multi-modal models that achieve impressive performance on diverse downstream tasks. We use VGGSound er to benchmark the audio-visual capabilities of the aforementioned models.</text>
<text><loc_259><loc_353><loc_452><loc_412>Audio-visual classification benchmarks Audio-visual classification is distinct from general video classification (e.g. on YouTube-8M [1]), as classes typically cover both audible and visible actions or events. Commonly used datasets for audio-visual classification include KineticsSound [8] sourced from the Kinetics dataset [41], FlickrSoundNet [11] scraped from Flickr, and AudioSet [29] and VGGSound [13], both sourced from YouTube.</text>
<text><loc_259><loc_414><loc_452><loc_450>Kinetics-Sound features manual labels of human actions, but covers only 34 classes. Flickr-SoundNet is much larger, but only a small subset is labelled. Similarly, only a small fraction of the roughly 2M AudioSet samples are annotated and have aligned audio and video.</text>
<page_footer><loc_243><loc_474><loc_257><loc_479>1028</page_footer>
<page_break>
<caption><loc_48><loc_148><loc_452><loc_174>Figure 2. Limitations of VGGSound. We show video frames from videos in the VGGSound test set along with their annotated label (grey) to demonstrate various limitations. A . VGGSound samples are labelled with a single class, yet many videos contain multiple distinct classes. B . Additionally, many classes partially overlap or are ambiguous. C . Some samples are labelled with classes that are not present in one of the modalities (i.e., the labelled class is not visible or audible).</caption>
<text><loc_48><loc_186><loc_241><loc_290>In contrast, VGGSound ensures audio-visual correspondence for around 200 000 samples and was curated with an automatic pipeline involving class-list generation, and auditory and visual content verification. The visual verification step ensures that a class is represented in the centre frame. The VEGAS dataset [88] provides better quality assurances for a small subset of AudioSet with only 10 classes. Visually Aligned Sounds [16] subsamples VEGAS and AudioSet after human verification, and Visual Sound [76] subsamples VGGSound using a multi-modal embedding model, both aiming for high audio-visual correspondence. Similarly, VGGSound-Sparse [37] is a subset of VGGSound with a focus on temporally and spatially sparse synchronisation signals (e.g., short loud noises).</text>
<text><loc_48><loc_292><loc_241><loc_328>Overall, VGGSound strikes the best balance between size, generality, and annotations, making it a common benchmark for audio-visual classification. We update VGGSound to sustain its usability for the development of the next generation of multi-modal foundation models.</text>
<section_header_level_1><loc_48><loc_336><loc_167><loc_343>3. Limitations of VGGSound</section_header_level_1>
<text><loc_48><loc_349><loc_241><loc_392>Since we are interested in the VGGSound dataset for benchmarking audio-visual multi-modal models, our analysis focuses on the VGGSound test set, 1 which consists of 15 446 video clips, each 10s long and labelled with one of 309 classes. We qualitatively identify several limitations of the VGGSound annotations outlined below and in Fig. 2.</text>
<text><loc_48><loc_396><loc_241><loc_439>Co-occurring classes While VGGSound's visual verification aimed to minimise multiple classes co-occurring in a clip, we find that most samples nevertheless clearly contain multiple classes, see Fig. 2A. In some cases, classes are temporally separated, e.g., showing male speech, man speaking and then cutting to</text>
<footnote><loc_57><loc_444><loc_224><loc_450>1 Although these issues most likely also apply to the training set.</footnote>
<text><loc_259><loc_186><loc_452><loc_253>footage of firing cannon . Most often, classes co-occur at the same time, sometimes for the entire duration of the video clip. Overlapping classes are often related, such as different instruments in a band or orchestra, but can also be entirely unrelated, e.g., donkey, ass braying co-occurring with playing violin . As additional empirical evidence, we provide a co-occurrence matrix computed using CAV-MAE [33], a state-of-the-art audio-visual model, in Appendix D.</text>
<text><loc_259><loc_259><loc_452><loc_378>Overlapping classes The issue of co-occurring classes is exacerbated by many of the 309 automatically generated classes partially overlapping in their definition, as illustrated in Fig. 2B. We found two pairs of synonymous classes: timpani and tympani and dog barking and dog bow-wow . Additionally, some classes are strict subclasses of others, such as the genderspecific versions of cattle mooing : cow lowing and bull bellowing ; or the more specific variants of people eating : people eating noodle and people eating apple . Finally, several classes commonly appear together, such as playing snare drum which is often played as part of a drum kit , or semantically similar concepts: running electric fan and air conditioning noise , and sloshing water and splashing water .</text>
<text><loc_259><loc_384><loc_452><loc_450>Modality misalignment Despite VGGSound's auditory and visual content verification, we find that many of the annotated classes are not visible or not audible, as shown in Fig. 2C. A large fraction of videos contains background music, voice-over and narration, or other background sounds like bird chirping, tweeting or cricket chirping without a visible source. Similarly, some videos contain visible but inaudible cues for classes like sea waves . Static images and slide shows accompa-</text>
<page_footer><loc_243><loc_474><loc_257><loc_479>1029</page_footer>
<page_break>
<picture><loc_48><loc_47><loc_449><loc_135><caption><loc_48><loc_142><loc_452><loc_169>Figure 3. Overview of VGGSound er . A . Most samples contain more than one label. B . More than a quarter of labels are audible but not visible. In contrast, only a tiny fraction is visible but not audible. C . Speech and bird sounds are the most common classes; more details can be found in Appendix B. D . Forty percent of the samples contain some combination of background music , voice over , and static image(s) , making the classification task significantly harder.</caption></picture>
<text><loc_48><loc_180><loc_241><loc_231>nied by music or other sounds are other frequent sources of misaligned modalities. Finally, some classes are misaligned by definition: wind noise is only audible and not visible. Overall, 48.43 % of the original VGGSound test samples have misaligned modalities. This finding challenges the widely held assumption that VGGSound has strong modality alignment [32, 47].</text>
<text><loc_48><loc_233><loc_241><loc_307>Other datasets, such as Visually Aligned Sounds, Visual Sound, and VGGSound-Sparse (see Sec. 2) omitted samples with misaligned modalities. In contrast, we contend that inaudible or invisible cues are common in natural videos and should be considered when benchmarking multi-modal models. We, therefore, place particular emphasis on crafting reliable modality annotations for all samples, allowing users to evaluate models on samples that guarantee modality alignment, and on those where classes are only visible or only audible (see Tab. 1).</text>
<text><loc_53><loc_316><loc_236><loc_345>Takeaway 1 VGGSound suffers from several issues: class co-occurrence not captured by single labels, overlapping class definitions, and modality misalignment, see Fig. 2.</text>
<section_header_level_1><loc_48><loc_354><loc_153><loc_364>4. Building VGGSound er</section_header_level_1>
<text><loc_48><loc_369><loc_241><loc_450><loc_259><loc_180><loc_452><loc_209>We propose a series of fixes for VGGSound's issues, ultimately resulting in the updated VGGSound er benchmark. We are not the first aiming to future-proof an existing benchmark: [12] analysed shortcomings of ImageNet [24], ultimately proposing switching to a multi-label classification task with additional manual labels. [28] similarly reannotated samples in MMLU [35, 36] to fix labelling errors. Both works inspire our approach to improve VGGSound. To deal with co-occurring classes, we switch to a multilabel classification setting. This effectively handles most overlapping class definitions: a strong model can assign a high probability to multiple classes, even if they partially overlap. This also allows us to ensure that synonymous classes, as well as subclasses and their superclass, always appear together in the ground-truth labels.</text>
<text><loc_259><loc_211><loc_452><loc_270>To deal with modality misalignment, we add a modality annotation to each label. For example, we can label a video as containing people clapping in the audio and containing playing volleyball in the audio and video data. We also add meta-labels to indicate whether a sample contains background music , voice over , or static image(s) to optionally treat these cases separately during evaluation.</text>
<text><loc_259><loc_272><loc_452><loc_286>We employ a pipeline similar to [12] to annotate multiple labels per sample, which we outline below.</text>
<text><loc_259><loc_290><loc_452><loc_349>Collecting proposals We create a gold standard reference set by labelling a small, randomly selected subset of VGGSound test samples with four in-house computer vision experts. The interface used for this first labelling step is shown in Appendix A. We extend the subset until each class is covered at least once, leading to a final size of 417 samples. Labels from different annotators are merged via a simple majority vote.</text>
<text><loc_259><loc_351><loc_452><loc_418>Given the gold standard set, we want to find a solid strategy for automatically generating label proposals which are shown to the humans labelling the test set. This should have a recall greater than 90 % while maximising precision compared to the gold standard labels to produce a small set of proposals with good label coverage. Our final strategy combines predictions from several state-of-the-art models with a manual heuristic to obtain 93% recall for an average of 30 proposals per sample, see Appendix A.</text>
<text><loc_259><loc_422><loc_452><loc_450>Human labelling We use Amazon Mechanical Turk to re-annotate the entire VGGSound test set. For each sample, we first ask annotators to indicate whether the video contains background music , voice over , or</text>
<page_footer><loc_243><loc_472><loc_257><loc_480>1030</page_footer>
<page_break>
<text><loc_48><loc_47><loc_241><loc_129>static image(s) . Then, annotators are asked to indicate for each label proposal whether the class is audible and/or visible . Finally, annotators can add a class if it is missing from the proposals. Annotators were paid the US minimum wage; the interface used is shown in Appendix A. We let annotators label the samples in batches of 20, each containing two gold standard samples as catch trials. We reject and re-annotate all batches with a catch trial F 1 -score below 25%. In addition, we obtain modality annotations for the original VGGSound labels and meta-classes. Further details are provided in Appendix A.</text>
<text><loc_48><loc_132><loc_241><loc_176>Final labels We merge all obtained annotations using majority voting. Additionally, we automatically add synonymous classes and superclasses for a given subclass, e.g., we add cattle mooing whenever cow lowing is in the set of labels. The full set of classes added this way is described in Appendix A.</text>
<text><loc_53><loc_185><loc_236><loc_214>Takeaway 2 We develop VGGSound er : A multi-label video classification benchmark extending VGGSound with human-annotated multi-labels, modality annotations, and meta-labels as summarised in Fig. 3.</text>
<section_header_level_1><loc_48><loc_224><loc_205><loc_231>5. Benchmarking audio-visual models</section_header_level_1>
<text><loc_48><loc_237><loc_241><loc_258>We use VGGSound er to benchmark four popular audiovisual embedding models and seven foundation models, and analyse their auditory and visual capabilities.</text>
<text><loc_48><loc_261><loc_241><loc_283>Models We evaluate the audio-visual embedding models CAV-MAE [33], DeepAVFusion [57], AV-Siam [49], and Equi-AV [43]. Those were finetuned on VGGSound.</text>
<text><loc_48><loc_284><loc_241><loc_350>Webenchmark several models from the closed-source Gemini family [73] in a zero-shot evaluation protocol. Furthermore, we use LLM-assisted evaluation to evaluate the following four open-source autoregressive foundation models : VideoLLaMA-2 [21], Unified-IO-2 [53], Panda-GPT [72], and Ola [52]. All models are evaluated in three modes: using unimodal-audio, unimodal-visual, or multi-modal (audio and visual) inputs. Further details about models and their evaluation are provided in Appendix C.1.</text>
<text><loc_48><loc_354><loc_241><loc_435>Metrics To benchmark the models on VGGSound er , we use multi-label classification metrics. For embedding models, all metrics are computed for the topk predictions, with k ∈ { 1 , 3 , 5 , 10 } . In contrast, prompting foundation models yields an unordered set of class predictions of varying size, and we compute only a single metric using the entire set. As a result, metrics are not directly comparable between embedding and foundation models. To get a sense of their relative performance, we report metrics for embedding models for k = 1 in the main text, matching the median number of predictions per sample for the foundation models.</text>
<text><loc_48><loc_437><loc_241><loc_450><loc_259><loc_47><loc_452><loc_91>For open-source models such as VideoLLaMA-2, Ola, Unified-IO-2, and Panda-GPT, we employ LLM-assisted evaluation [55, 81], in which the Qwen-3 model [82] is tasked to assess the correspondence between model outputs and target classes. Closed-source models from the Gemini family are evaluated by providing the full list of 309 classes as input. Further details on the evaluation procedures and exact prompts are provided in Appendix C.</text>
<text><loc_259><loc_93><loc_452><loc_114>Subset accuracy compares the predicted label set to the ground-truth label set and reports the fraction of samples for which they match. This is our strictest metric.</text>
<text><loc_259><loc_115><loc_452><loc_129>F 1 -score is the harmonic mean of precision and recall. It is strictly larger than the subset accuracy.</text>
<text><loc_259><loc_130><loc_452><loc_166>Hit reports the fraction of samples for which any of the predicted labels are part of the ground-truth label set. This is the most lenient metric which is strictly larger than the F 1 -score. We include this metric for ease of comparison to the 'ReaL-Accuracy' used in [12].</text>
<text><loc_259><loc_168><loc_452><loc_280>All metrics are computed separately for each input modality (audio, video, and audio-visual) and label modality. We use lowercase symbols a, v, and av to indicate the input modality: audio-only, visual-only, or audio-visual inputs, respectively. For label modality, we use uppercase symbols A , V , and AV , referring to the subsets of the benchmark with audible, visible, and audio-visual labels. We further include A ¬ V (audible but not visible) and V ¬ A (visible but not audible) to analyze unaligned cues. For clarity, we define shorthand notations such as a = a ( A ) to denote the model's performance on the audible subset A using only audio input. Analogously, v = v ( V ) and av = av ( AV ) refer to videoonly and audio-visual performance on their respective label subsets. Furthermore, we use micro-aggregation to balance the contribution from each class.</text>
<text><loc_259><loc_281><loc_452><loc_302>Weadditionally measure the negative impact of using multimodal inputs. In particular, µ is a new metric we propose to measure a model's modality confusion ( µ ). We define it as</text>
<formula><loc_263><loc_307><loc_452><loc_323></formula>
<text><loc_259><loc_327><loc_452><loc_417>where M ∈ [ A,V,A ∩ V ] and their associated modality inputs are m ∈ [ a, v ] , correct/wrong is determined as in the Hit score (with k = 1 for embedding models). N total refers to the total number of samples. µ measures the fraction of samples a model correctly classified given an input modality but got wrong when using both modalities simultaneously. Weadditionally report µ A ∩ V as the percentage of samples a model could solve in either modality unimodally but could not solve multi-modally. In other words, the modality confusion µ captures how frequently a model is distracted by an additional input modality, which can indicate the unsuccessful merging of modalities.</text>
<text><loc_264><loc_426><loc_447><loc_447>Takeaway 3 We propose a new metric, modality confusion µ , that measures how frequently a model is distracted by an additional input modality; see Eq. (1).</text>
<page_footer><loc_243><loc_474><loc_257><loc_479>1031</page_footer>
<page_break>
<otsl><loc_48><loc_45><loc_451><loc_164><ecel><ched>Subset Accuracy ↑<lcel><lcel><ecel><ecel><ecel><ecel><ecel><ched>Hit ↑<lcel><lcel><ched>µ ↓<lcel><lcel><nl><ched>Model<ched>a<ched>v<ched>av<ched>a<ched>v<ched>F 1 av<ched>↑ a( A ¬ V )<ched>v( V ¬ A )<ched>a<ched>v<ched>av<ched>µ A<ched>µ V<ched>µ A ∩ V<nl><srow>Embedding Models<lcel><lcel><lcel><lcel><lcel><lcel><lcel><lcel><lcel><lcel><lcel><lcel><lcel><lcel><nl><rhed>CAV-MAE<fcel>13.19<fcel>19.23<fcel>24.49<fcel>34.46<fcel>34.91<fcel>42.62<fcel>13.94<fcel>19.00<fcel>62.29<fcel>53.44<fcel>64.17<fcel>3.58<fcel>6.43<fcel>0.77<nl><rhed>DeepAVFusion<fcel>10.19<fcel>11.10<fcel>21.53<fcel>25.31<fcel>21.29<fcel>37.35<fcel>10.37<fcel>10.55<fcel>45.77<fcel>32.61<fcel>56.27<fcel>3.74<fcel>3.93<fcel>0.17<nl><rhed>Equi-AV<fcel>11.60<fcel>10.52<fcel>20.00<fcel>29.39<fcel>20.42<fcel>34.69<fcel>12.55<fcel>10.65<fcel>53.12<fcel>31.26<fcel>52.24<fcel>6.97<fcel>7.13<fcel>1.38<nl><rhed>AV-Siam<fcel>12.79<fcel>19.75<fcel>22.83<fcel>33.30<fcel>35.41<fcel>39.43<fcel>12.90<fcel>18.21<fcel>60.19<fcel>54.20<fcel>59.36<fcel>9.36<fcel>8.80<fcel>3.58<nl><srow>Closed-source Foundation Models<lcel><lcel><lcel><lcel><lcel><lcel><lcel><lcel><lcel><lcel><lcel><lcel><lcel><lcel><nl><rhed>Gemini 1.5 Flash<fcel>1.78<fcel>14.44<fcel>16.44<fcel>14.49<fcel>36.98<fcel>42.52<fcel>15.61<fcel>21.61<fcel>32.73<fcel>47.36<fcel>59.10<fcel>10.22<fcel>4.25<fcel>0.77<nl><rhed>Gemini 1.5 Pro<fcel>3.05<fcel>20.86<fcel>22.53<fcel>19.26<fcel>49.73<fcel>53.74<fcel>17.73<fcel>22.90<fcel>35.03<fcel>69.23<fcel>75.42<fcel>2.09<fcel>4.85<fcel>0.57<nl><rhed>Gemini 2.0 Flash<fcel>1.85<fcel>12.54<fcel>12.69<fcel>11.80<fcel>34.08<fcel>36.45<fcel>6.19<fcel>18.90<fcel>18.51<fcel>43.83<fcel>47.72<fcel>2.39<fcel>5.43<fcel>1.00<nl><srow>Open-source Foundation Models<lcel><lcel><lcel><lcel><lcel><lcel><lcel><lcel><lcel><lcel><lcel><lcel><lcel><lcel><nl><rhed>VideoLLaMA 2<fcel>12.86<fcel>19.85<fcel>24.47<fcel>38.87<fcel>47.82<fcel>52.35<fcel>20.34<fcel>28.08<fcel>58.91<fcel>52.02<fcel>59.80<fcel>12.72<fcel>5.46<fcel>2.95<nl><rhed>Unified-IO 2<fcel>11.94<fcel>11.56<fcel>25.61<fcel>35.31<fcel>27.92<fcel>48.89<fcel>21.38<fcel>16.53<fcel>54.39<fcel>31.05<fcel>65.11<fcel>8.70<fcel>5.16<fcel>1.79<nl><rhed>PandaGPT<fcel>3.19<fcel>4.19<fcel>5.46<fcel>18.73<fcel>18.56<fcel>20.85<fcel>16.82<fcel>14.40<fcel>21.08<fcel>17.01<fcel>18.82<fcel>7.59<fcel>5.90<fcel>2.47<nl><rhed>OLA<fcel>14.11<fcel>8.69<fcel>18.19<fcel>47.70<fcel>24.85<fcel>46.48<fcel>40.44<fcel>13.45<fcel>59.05<fcel>24.57<fcel>51.51<fcel>15.47<fcel>6.80<fcel>2.49<nl><caption><loc_48><loc_169><loc_452><loc_204>Table 2. Audio-visual video classification results on VGGSound er . We report multi-label classification metrics (subset accuracy, F 1 -score, Hit accuracy, modality confusion ( µ ) for audioa ( A ) , visual v ( V ) , audio-visual av ( AV ) , audio-only a ( A ¬ V ) and video-only -v ( V ¬ A ) inputs. The embedding models CAV-MAE, DeepAVFusion, and Equi-AV were finetuned on the VGGSound training set. We report metrics for k = 1 here and for other k in Appendix D. The closed sourced multi-modal foundation models Gemini and open-sourced models use a zero-shot evaluation protocol and LLM-assisted protocol respectively.</caption></otsl>
<section_header_level_1><loc_48><loc_214><loc_188><loc_221>5.1. Re-evaluating the state of the art</section_header_level_1>
<text><loc_48><loc_225><loc_241><loc_239>We present the benchmark performance of state-of-the-art audio-visual models in Tab. 2.</text>
<text><loc_48><loc_242><loc_241><loc_354>Overall performance Unsurprisingly, all models perform best with access to both input modalities ( AV ). Across all metrics, both open- and closed-source general-purpose models perform comparably to the purpose-built embedding models CAV-MAE and AV-Siam. This indicates that foundation models have reached - and for some modalities exceeded - the performance of specialised models. However, the embedding models finetuned on VGGSound generally have stronger unimodal performance with audio inputs ( A ) compared to visual inputs ( V ), which is in line with their pretraining on AudioSet. Interestingly, this trend is reversed for most foundation models, which seem to be biased towards visual inputs, with unimodal video performance ( V ) being substantially higher than unimodal audio performance ( A ).</text>
<text><loc_53><loc_365><loc_236><loc_401>Takeaway 4 Foundation models perform comparably to finetuned embedding models . Embedding models more heavily rely on audio cues than on visual ones, while foundation models exploit visible cues rather than audible ones, see Tab. 2.</text>
<text><loc_48><loc_414><loc_241><loc_450><loc_259><loc_214><loc_452><loc_296>Modality confusion The modality confusion score µ shows that, for all models, a notable fraction of test samples (4-11%) were misclassified when an additional modality was included-despite being correctly classified with unimodal input. Furthermore, for all models, a small portion of test samples is not solvable multi-modally even though they were solvable in both modalities alone ( µ A ∩ V ). This insight is made possible by VGGSound er 's per-label modality annotations and shows that all models are susceptible to being distracted given an additional modality. This is a concerning issue for multi-modal models since they should preserve unimodal capabilities when adding a second modality. Being able to evaluate this behaviour on the VGGSound er benchmark is a first step towards enabling the development of mitigation strategies, eventually resulting in stronger audio-visual models.</text>
<text><loc_264><loc_305><loc_447><loc_326>Takeaway 5 Our modality confusion score reveals that all models are negatively impacted by additional modalities for a substantial amount of samples (see Tab. 2).</text>
<text><loc_259><loc_339><loc_452><loc_398>Performance across modalities Fig. 4 shows the performance profiles across modalities. At first glance, we can see that VideoLLaMA-2's performance is well balanced for different input modalities, while models from the Gemini family distinctly underperform on audio inputs. In contrast, embedding models exhibit a moderate balance across modalities, with DeepAVFusion and EquiAV showing slight underperformance for visual input.</text>
<text><loc_259><loc_399><loc_452><loc_450>As Fig. 4 also illustrates, profiling of this kind is enabled through the modality annotations in VGGSound er . In contrast, VGGSound assumed that all classes are perceivable in both modalities, and did not account for background sounds. This resulted in consistent under-evaluation of foundation models (that were not finetuned on VGGSound) for audio inputs.</text>
<page_footer><loc_243><loc_472><loc_257><loc_480>1032</page_footer>
<page_break>
<text><loc_48><loc_47><loc_241><loc_99>In addition to the radar plot in Fig. 4, we provide results on VGGSound in Appendix D, showing that all models have substantially lower performance than their hit scores in Tab. 2. This confirms that many model predictions were incorrectly flagged as false positives in VGGSound due to the incomplete ground-truth labels, painting a distorted picture of models' limitations.</text>
<picture><loc_53><loc_106><loc_235><loc_232><caption><loc_48><loc_237><loc_241><loc_279>Figure 4. VGGSound er more accurately captures model performance across input modalities . We show the Hit score on VGGSound er and accuracy on VGGSound, normalised by the permodel maximum performance on each benchmark. Specifically for foundation models, we observe a significant difference in performance between VGGSound and VGGSound er .</caption></picture>
<text><loc_53><loc_294><loc_236><loc_315>Takeaway 6 VGGSound er 's more complete groundtruth labels allow for more accurate, modality-specific profiling of model performance (see Fig. 4).</text>
<section_header_level_1><loc_48><loc_327><loc_217><loc_334>5.2. Performance analysis using meta-classes</section_header_level_1>
<text><loc_48><loc_339><loc_241><loc_382>VGGSound er includes annotations of three meta-classes for each sample: background music indicates whether samples contain music without a visible source, voice over similarly marks speech without a visible source, and static image(s) flags that the visual stream consists of one or only a few static visual frames.</text>
<text><loc_48><loc_384><loc_241><loc_450>These new meta-classes allow us to evaluate the model behaviour in challenging scenarios where information from one modality dominates. We consider the performance difference between samples that do contain a meta-label and samples that do not contain it. Positive numbers indicate that the models perform better on the subset with metalabels. In Tab. 3, we summarise the main findings, focussing on F 1 -score as the most balanced multi-label metric. Additional results are provided in Appendix D.</text>
<text><loc_259><loc_47><loc_452><loc_91>Background music All models perform worse on video samples containing background music. This indicates that it is challenging to decouple background audio from the rest of the video. The evaluated models are not good at differentiating between sound sources without visual cues, e.g. predicting different instruments in the background music.</text>
<text><loc_259><loc_94><loc_452><loc_153>Voice over In contrast to background music, we observe a clear difference between embedding models and foundation models for samples with voice-over. While the audio classification performance of embedding models drops substantially. This drop is only slight for VideoLLaMA-2 and Unified-IO 2, and the performance of other foundation models even improves. This indicates that the foundation models are less distracted by voice-over.</text>
<text><loc_259><loc_156><loc_452><loc_200>Static image(s) The impact of static images is more nuanced: First, audio classification performance improves for embedding models while it decreases for the foundation models. This shows that the purpose-built, VGGSoundfinetuned embedding models can more accurately predict specific sounds in the absence of other cues.</text>
<text><loc_259><loc_202><loc_452><loc_223>Second, visual classification performance on static images drops for all models, suggesting that models rely on rich temporal cues to make accurate predictions.</text>
<text><loc_259><loc_224><loc_452><loc_291>Third, the subset accuracy (i.e. exact label set matches) for uni-modal predictions shows that foundation models perform better in visual than audio classification with and without static images. In contrast, embedding models perform better in audio classification on static images but worse in visual classification for other samples. This suggests that non-static samples form a challenging subset for embedding models, where a model needs to favour one modality over another one to make a correct prediction.</text>
<text><loc_259><loc_294><loc_452><loc_345>Samples without any meta-label When comparing the model performance on samples without background music, static images, or voice-over annotations (column neither in Tab. 3) to the performance on the entire test set, we see a performance gain (here negative numbers indicate an increased F 1 score). This finding concludes that these three categories form challenging subsets of the dataset.</text>
<text><loc_264><loc_356><loc_447><loc_384>Takeaway 7 Samples with background music , static images(s) , and voice over provide distinct challenges for each model (see Tab. 3). This highlights VGGSound er 's value for comprehensive model evaluation.</text>
<section_header_level_1><loc_259><loc_395><loc_390><loc_402>5.3. Impact of VGGSound er labels</section_header_level_1>
<text><loc_259><loc_407><loc_452><loc_450>Our relabelling pipeline adds two types of labels to those in VGGSound: (1) automatically generated labels based on synonymous classes and sub-/superclasses, and (2) humancurated labels. In Tab. 4 , we ablate the impact of each type of added labels in terms of the relative performance gains (Hit score). A complete breakdown of the effects across all</text>
<page_footer><loc_243><loc_472><loc_257><loc_480>1033</page_footer>
<page_break>
<otsl><loc_48><loc_45><loc_452><loc_153><ecel><ched>background music<lcel><lcel><ched>voice over<ched>static image(s)<lcel><lcel><lcel><lcel><lcel><ecel><ched>neither<ecel><nl><ecel><ched>∆ F 1<lcel><lcel><ched>∆ F 1<ched>∆ F 1<lcel><ched>Sub. Acc.<fcel>w/<ched>Sub. Acc. w/o<lcel><ched>∆ F 1<lcel><lcel><nl><ched>Model<ched>a<ched>v<ched>av<ched>a<ched>a<ched>v<ched>a<ched>v<ched>a<ched>v<ched>a<ched>v<ched>av<nl><srow>Embedding Models<ecel><ecel><ecel><ecel><ecel><ecel><ecel><ecel><ecel><ecel><ecel><ecel><ecel><nl><rhed>CAV-MAE<fcel>-0.66<fcel>-0.59<fcel>-0.65<fcel>-1.13<fcel>0.31<fcel>-0.28<fcel>12.73<fcel>19.22<fcel>11.98<fcel>19.21<fcel>-1.00<fcel>-1.27<fcel>-1.24<nl><rhed>DeepAVFusion<fcel>-0.71<fcel>-0.80<fcel>-0.70<fcel>-1.25<fcel>0.28<fcel>-0.18<fcel>9.79<fcel>10.81<fcel>9.30<fcel>10.81<fcel>-1.27<fcel>-1.01<fcel>-1.36<nl><rhed>Equi-AV<fcel>-0.79<fcel>-0.42<fcel>-0.41<fcel>-0.97<fcel>0.28<fcel>-0.23<fcel>11.12<fcel>10.45<fcel>10.49<fcel>10.45<fcel>-0.97<fcel>-0.97<fcel>-0.89<nl><rhed>AV-Siam<fcel>-0.73<fcel>-0.72<fcel>-0.82<fcel>-1.08<fcel>0.34<fcel>-0.26<fcel>12.34<fcel>19.53<fcel>11.57<fcel>19.52<fcel>-1.01<fcel>-1.34<fcel>-1.39<nl><srow>Closed-source Foundation Models<ecel><ecel><ecel><ecel><ecel><ecel><ecel><ecel><ecel><ecel><ecel><ecel><ecel><nl><rhed>Gemini 1.5 Flash<fcel>-0.22<fcel>-0.41<fcel>-0.69<fcel>2.08<fcel>-0.36<fcel>-0.25<fcel>1.66<fcel>14.35<fcel>1.68<fcel>14.39<fcel>1.40<fcel>-1.10<fcel>-1.62<nl><rhed>Gemini 1.5 Pro<fcel>-0.36<fcel>-0.61<fcel>-0.94<fcel>2.48<fcel>-0.31<fcel>-0.24<fcel>2.83<fcel>20.85<fcel>2.87<fcel>20.80<fcel>1.64<fcel>-0.89<fcel>-1.09<nl><rhed>Gemini 2.0 Flash<fcel>-0.09<fcel>-0.32<fcel>-0.58<fcel>0.03<fcel>0.13<fcel>-0.26<fcel>1.70<fcel>12.33<fcel>1.53<fcel>12.40<fcel>-0.03<fcel>-1.07<fcel>-1.22<nl><srow>Open-source Foundation Models<ecel><ecel><ecel><ecel><ecel><ecel><ecel><ecel><ecel><ecel><ecel><ecel><ecel><nl><rhed>VideoLLaMA 2<fcel>-0.47<fcel>-0.72<fcel>-0.84<fcel>-0.54<fcel>0.27<fcel>-0.31<fcel>12.55<fcel>19.64<fcel>12.04<fcel>19.70<fcel>-0.47<fcel>-1.26<fcel>-1.33<nl><rhed>Unified-IO 2<fcel>-1.20<fcel>0.18<fcel>-0.65<fcel>-0.68<fcel>0.12<fcel>-0.21<fcel>11.39<fcel>11.89<fcel>10.88<fcel>12.00<fcel>-1.07<fcel>-0.15<fcel>-1.17<nl><rhed>PandaGPT<fcel>-1.13<fcel>-0.15<fcel>-0.42<fcel>0.56<fcel>-0.22<fcel>-0.16<fcel>2.94<fcel>4.27<fcel>2.91<fcel>4.24<fcel>-0.60<fcel>-0.15<fcel>-0.65<nl><rhed>OLA<fcel>-2.16<fcel>0.09<fcel>-0.44<fcel>1.50<fcel>-0.48<fcel>-0.18<fcel>13.03<fcel>8.88<fcel>12.88<fcel>8.89<fcel>-0.80<fcel>-0.13<fcel>-0.06<nl><caption><loc_48><loc_159><loc_452><loc_178>Table 3. Summary of performance differences in the presence/absence of meta-classes. Difference in F 1 scores ( ∆ F 1 ) for audiovisual video classification on VGGSound er between videos with a meta-class and those without it. Positive numbers ( ∆ ) indicate better performance when the meta-class is present. Additional results are provided in Appendix D.</caption></otsl>
<otsl><loc_48><loc_187><loc_240><loc_222><ecel><ched>Human labels ↑<lcel><lcel><ched>Auto labels ↑<lcel><lcel><nl><ched>Model<ched>A<ched>V<ched>AV<ched>A<ched>V<ched>AV<nl><rhed>Gemini 1.5 Flash<fcel>29.28<fcel>14.59<fcel>16.36<fcel>0.48<fcel>0.93<fcel>1.51<nl><rhed>Gemini 1.5 Pro<fcel>28.61<fcel>25.52<fcel>27.63<fcel>0.31<fcel>1.99<fcel>2.10<nl><rhed>Gemini 2.0 Flash<fcel>8.80<fcel>12.16<fcel>11.13<fcel>0.22<fcel>1.12<fcel>1.28<nl><caption><loc_48><loc_229><loc_241><loc_262>Table 4. Impact of added labels using different strategies in VGGSound er . We show the change in multi-label classification performance ( ∆ Hit) when adding automatically added (Auto) or human-annotated (Human) labels to VGGSound, and compare to the original VGGSound data.</caption></otsl>
<text><loc_48><loc_273><loc_241><loc_324>models and metrics is provided in Appendix D. While performance is consistently higher with automatically added labels (Auto), the increase is noticeably smaller than that for human-curated labels. Paired with the observation that models do frequently predict correct classes that were not part of the original VGGSound label set, this indicates that human-curated labels better cover the ground truth.</text>
<text><loc_53><loc_336><loc_236><loc_364>Takeaway 8 Automatically added labels are an important step, but human-curated labels have a bigger effect on eliminating incorrectly flagged false positives, underscoring the value of accurate human annotation.</text>
<section_header_level_1><loc_48><loc_377><loc_102><loc_384>6. Discussion</section_header_level_1>
<text><loc_48><loc_391><loc_241><loc_450><loc_259><loc_189><loc_452><loc_203>Choice of VGGSound as base dataset VGGSound is commonly used to evaluate audio-visual models on the multimodal video classification task. As it is currently the most suitable testbed for audio-visual classification tasks (due to its size, diversity of categories, non-constrained setting, and relatively strong audio-visual correspondence), it serves as an optimal starting point for our substantially improved VGGSound er benchmark with a multi-label evaluation pro- tocol for foundation models that makes the benchmark suitable for meaningful evaluation.</text>
<text><loc_259><loc_206><loc_452><loc_280>Multi-label vs single-label classification Video content is inherently complex, often containing multiple co-occurring objects and actions both within and across modalities. This makes it unlikely that a given clip belongs to just one class as is the case in the single-label classification task. Therefore, our VGGSound er extends the VGGSound test set to multi-label classification. Unlike models trained on a narrow single-label dataset, foundation models develop versatile representations. Multi-label evaluation thus also aligns very naturally with this capability.</text>
<section_header_level_1><loc_259><loc_288><loc_316><loc_295>7. Conclusion</section_header_level_1>
<text><loc_259><loc_301><loc_452><loc_352>We introduced an modality-aware evaluation set for audiovisual foundation models. VGGSound er builds on the widely used VGGSound dataset by adding: (a) comprehensive human annotations for missing classes, (b) specifying modality information per label, (c) introducing specialised meta-labels for frequently occurring real-world challenges, and (d) using heuristic methods to improve label quality.</text>
<text><loc_259><loc_354><loc_452><loc_443>Through our newly introduced metric, modality confusion, we observe that incorporating additional modalities does not necessarily yield better results. Models often become more confused on a substantial subset of test samples. Furthermore, finetuned embedding models tend to rely heavily on audio cues, while foundation models depend more on visual information. Additionally, our meta-label analysis highlights distinct challenges across various specialised yet commonly occurring scenarios such as background music, static images, and voice-overs. Overall, we hope that the VGGSound er benchmark will advance the evaluation and development of foundational audio-visual models.</text>
<page_footer><loc_243><loc_472><loc_257><loc_480>1034</page_footer>
<page_break>
<section_header_level_1><loc_48><loc_46><loc_129><loc_54>Acknowledgements</section_header_level_1>
<text><loc_48><loc_59><loc_241><loc_103>The authors would like to thank Felix F¨ orster, Sayak Mallick, and Prasanna Mayilvahananan for their help with data annotation, Felix F¨ orster and Monica Riedler for proofreading the paper, and Thomas Klein and Shyamgopal Karthik for their help in setting up MTurk. They also thank numerous MTurk workers for labelling.</text>
<text><loc_48><loc_105><loc_241><loc_194>This work was in part supported by the BMBF (FKZ: 01IS24060, 01I524085B, 01IS18039A), the DFG (SFB 1233, project number: 276693517), and the Open Philanthropy Foundation funded by the Good Ventures Foundation. WB acknowledges financial support via an Emmy Noether Grant funded by the German Research Foundation (DFG) under grant no. BR 6382/1-1. WB is a member of the Machine Learning Cluster of Excellence, EXC number 2064/1 - Project number 390727645. This research utilised compute resources at the T¨ ubingen Machine Learning Cloud, DFG FKZ INST 37/1057-1 FUGG. The authors thank the IMPRS-IS for supporting TW.</text>
<section_header_level_1><loc_48><loc_203><loc_93><loc_210>References</section_header_level_1>
<ordered_list><list_item><loc_51><loc_215><loc_241><loc_248>Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadarajan, and Sudheendra Vijayanarasimhan. Youtube-8m: A largescale video classification benchmark. arXiv preprint arXiv:1609.08675 , 2016.</list_item>
<list_item><loc_51><loc_251><loc_241><loc_270>Triantafyllos Afouras, Joon Son Chung, Andrew Senior, Oriol Vinyals, and Andrew Zisserman. Deep audio-visual speech recognition. IEEE TPAMI , 2018.</list_item>
<list_item><loc_51><loc_272><loc_241><loc_291>Triantafyllos Afouras, Yuki M Asano, Francois Fagan, Andrea Vedaldi, and Florian Metze. Self-supervised object detection from audio-visual correspondence. In ECCV , 2020.</list_item>
<list_item><loc_51><loc_294><loc_241><loc_313>Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman. Asr is all you need: Cross-modal distillation for lip reading. In ICASSP , 2020.</list_item>
<list_item><loc_51><loc_315><loc_241><loc_335>Triantafyllos Afouras, Andrew Owens, Joon Son Chung, and Andrew Zisserman. Self-supervised learning of audio-visual objects from video. In ECCV , 2020.</list_item>
<list_item><loc_51><loc_337><loc_241><loc_356>Humam Alwassel, Dhruv Mahajan, Lorenzo Torresani, Bernard Ghanem, and Du Tran. Self-supervised learning by cross-modal audio-video clustering. In NeurIPS , 2020.</list_item>
<list_item><loc_51><loc_359><loc_241><loc_378>Shivangi Aneja, Justus Thies, Angela Dai, and Matthias Nießner. Facetalk: Audio-driven motion diffusion for neural parametric head models. In CVPR , 2024.</list_item>
<list_item><loc_51><loc_380><loc_241><loc_392>Relja Arandjelovi´ c and Andrew Zisserman. Look, listen and learn. In ICCV , 2017.</list_item>
<list_item><loc_51><loc_395><loc_241><loc_407>Relja Arandjelovic and Andrew Zisserman. Objects that sound. In ECCV , 2018.</list_item>
<list_item><loc_48><loc_409><loc_241><loc_429>Yuki M. Asano, Mandela Patrick, Christian Rupprecht, and Andrea Vedaldi. Labelling unlabelled videos from scratch with multi-modal self-supervision. In NeurIPS , 2020.</list_item>
<list_item><loc_48><loc_431><loc_241><loc_450>Yusuf Aytar, Carl Vondrick, and Antonio Torralba. Soundnet: Learning sound representations from unlabeled video. In NeurIPS , 2016.</list_item>
<list_item><loc_259><loc_48><loc_452><loc_67>Lucas Beyer, Olivier J H´ enaff, Alexander Kolesnikov, Xiaohua Zhai, and A¨ aron van den Oord. Are we done with imagenet? arXiv preprint arXiv:2006.07159 , 2020.</list_item>
<list_item><loc_259><loc_69><loc_452><loc_88>Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. Vggsound: A large-scale audio-visual dataset. In ICASSP , 2020.</list_item>
<list_item><loc_259><loc_90><loc_452><loc_110>Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi, and Andrew Zisserman. Audio-visual synchronisation in the wild. In BMVC , 2021.</list_item>
<list_item><loc_259><loc_112><loc_452><loc_131>Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi, and Andrew Zisserman. Localizing visual sounds the hard way. In CVPR , 2021.</list_item>
<list_item><loc_259><loc_133><loc_452><loc_159>Peihao Chen, Yang Zhang, Mingkui Tan, Hongdong Xiao, Deng Huang, and Chuang Gan. Generating visually aligned sound from videos. IEEE Transactions on Image Processing , 2020.</list_item>
<list_item><loc_259><loc_161><loc_452><loc_188>Sanyuan Chen, Yu Wu, Chengyi Wang, Shujie Liu, Daniel Tompkins, Zhuo Chen, and Furu Wei. Beats: Audio pre-training with acoustic tokenizers. arXiv preprint arXiv:2212.09058 , 2022.</list_item>
<list_item><loc_259><loc_190><loc_452><loc_209>Yanbei Chen, Yongqin Xian, A. Sophia Koepke, Ying Shan, and Zeynep Akata. Distilling audio-visual knowledge by compositional contrastive learning. In CVPR , 2021.</list_item>
<list_item><loc_259><loc_211><loc_452><loc_237>Ziyang Chen, Prem Seetharaman, Bryan Russell, Oriol Nieto, David Bourgin, Andrew Owens, and Justin Salamon. Video-guided foley sound generation with multimodal controls. In CVPR , 2025.</list_item>
<list_item><loc_259><loc_239><loc_452><loc_265>Ying Cheng, Ruize Wang, Zhihao Pan, Rui Feng, and Yuejie Zhang. Look, listen, and attend: Co-attention network for self-supervised audio-visual representation learning. In ACM MM , 2020.</list_item>
<list_item><loc_259><loc_268><loc_452><loc_301>Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476 , 2024.</list_item>
<list_item><loc_259><loc_303><loc_452><loc_336>Wei-Lin Chiang, Zhuohan Li, Ziqing Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. 2023.</list_item>
<list_item><loc_259><loc_338><loc_452><loc_350>Joon Son Chung and Andrew Zisserman. Out of time: automated lip sync in the wild. In ACCV , 2016.</list_item>
<list_item><loc_259><loc_353><loc_452><loc_372>Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR , 2009.</list_item>
<list_item><loc_259><loc_374><loc_452><loc_393>Joshua P Ebeneze, Yongjun Wu, Hai Wei, Sriram Sethuraman, and Zongyi Liu. Detection of audio-video synchronization errors via event detection. In ICASSP , 2021.</list_item>
<list_item><loc_259><loc_395><loc_452><loc_414>Chuang Gan, Deng Huang, Peihao Chen, Joshua B Tenenbaum, and Antonio Torralba. Foley music: Learning to generate music from videos. In ECCV , 2020.</list_item>
<list_item><loc_259><loc_417><loc_452><loc_429>Ruohan Gao and Kristen Grauman. Co-separating sounds of visual objects. In ICCV , 2019.</list_item>
<list_item><loc_259><loc_431><loc_452><loc_450>Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad</list_item>
</ordered_list>
<page_footer><loc_243><loc_474><loc_257><loc_479>1035</page_footer>
<page_break>
<unordered_list><list_item><loc_64><loc_48><loc_241><loc_74>Reza Ghasemi Madani, Claire Barale, Robert McHardy, Joshua Harris, Jean Kaddour, Emile van Krieken, and Pasquale Minervini. Are we done with mmlu? arXiv preprint arXiv:2406.04127 , 2024.</list_item>
<list_item><loc_48><loc_76><loc_241><loc_102>Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and humanlabeled dataset for audio events. In ICASSP , 2017.</list_item>
<list_item><loc_48><loc_104><loc_241><loc_130>Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In CVPR , 2023.</list_item>
<list_item><loc_48><loc_133><loc_241><loc_145>Shir Goldstein and Yael Moses. Guitar music transcription from silent video. In BMVC , 2018.</list_item>
<list_item><loc_48><loc_147><loc_241><loc_166>Yuan Gong, Alexander H Liu, Andrew Rouditchenko, and James Glass. Uavm: Towards unifying audio and visual models. IEEE Signal Processing Letters , 2022.</list_item>
<list_item><loc_48><loc_168><loc_241><loc_194>Yuan Gong, Andrew Rouditchenko, Alexander H Liu, David Harwath, Leonid Karlinsky, Hilde Kuehne, and James Glass. Contrastive audio-visual masked autoencoder. In ICLR , 2023.</list_item>
<list_item><loc_48><loc_197><loc_241><loc_223>Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. A survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594 , 2024.</list_item>
<list_item><loc_48><loc_225><loc_241><loc_244>Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. Aligning ai with shared human values. In ICLR , 2021.</list_item>
<list_item><loc_48><loc_246><loc_241><loc_272>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In ICLR , 2021.</list_item>
<list_item><loc_48><loc_275><loc_241><loc_294>Vladimir Iashin, Weidi Xie, Esa Rahtu, and Andrew Zisserman. Sparse in space and time: Audio-visual synchronisation with trainable selectors. In BMVC , 2022.</list_item>
<list_item><loc_48><loc_296><loc_241><loc_315>Vladimir Iashin, Weidi Xie, Esa Rahtu, and Andrew Zisserman. Sparse in space and time: Audio-visual synchronisation with trainable selectors. In BMVC , 2022.</list_item>
<list_item><loc_48><loc_317><loc_241><loc_336>Vladimir Iashin, Weidi Xie, Esa Rahtu, and Andrew Zisserman. Synchformer: Efficient synchronization from sparse cues. In ICASSP , 2024.</list_item>
<list_item><loc_48><loc_339><loc_241><loc_358>Amir Jamaludin, Joon Son Chung, and Andrew Zisserman. You said that?: Synthesising talking faces from audio. IJCV , 2019.</list_item>
<list_item><loc_48><loc_360><loc_241><loc_393>Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950 , 2017.</list_item>
<list_item><loc_48><loc_395><loc_241><loc_414>Naji Khosravan, Shervin Ardeshir, and Rohit Puri. On attention modules for audio-visual synchronization. In CVPR Workshop , 2019.</list_item>
<list_item><loc_48><loc_417><loc_241><loc_436>Jongsuk Kim, Hyeongkeun Lee, Kyeongha Rho, Junmo Kim, and Joon Son Chung. Equiav: Leveraging equivariance for audio-visual contrastive learning. In ICML , 2024.</list_item>
<list_item><loc_48><loc_438><loc_241><loc_450>A. Sophia Koepke, Olivia Wiles, and Andrew Zisserman. Visual pitch estimation. In SMC , 2019.</list_item>
<list_item><loc_259><loc_48><loc_452><loc_67>A Sophia Koepke, Olivia Wiles, Yael Moses, and Andrew Zisserman. Sight to sound: An end-to-end approach for visual piano transcription. In ICASSP , 2020.</list_item>
<list_item><loc_259><loc_70><loc_452><loc_89>Bruno Korbar, Du Tran, and Lorenzo Torresani. Cooperative learning of audio and video models from self-supervised synchronization. In NeurIPS , 2018.</list_item>
<list_item><loc_259><loc_91><loc_452><loc_117>Sangho Lee, Jiwan Chung, Youngjae Yu, Gunhee Kim, Thomas Breuel, Gal Chechik, and Yale Song. Acav100m: Automatic curation of large-scale datasets for audio-visual video representation learning. In ICCV , 2021.</list_item>
<list_item><loc_259><loc_120><loc_452><loc_139>Guangyao Li, Yake Wei, Yapeng Tian, Chenliang Xu, JiRong Wen, and Di Hu. Learning to answer questions in dynamic audio-visual scenarios. In CVPR , 2022.</list_item>
<list_item><loc_259><loc_142><loc_452><loc_154>Yan-Bo Lin and Gedas Bertasius. Siamese vision transformers are scalable audio-visual learners. In ECCV , 2024.</list_item>
<list_item><loc_259><loc_157><loc_452><loc_176>Yan-Bo Lin and Yu-Chiang Frank Wang. Audiovisual transformer with instance attention for audio-visual event localization. In ACCV , 2020.</list_item>
<list_item><loc_259><loc_179><loc_452><loc_198>Yan-Bo Lin, Yu-Jhe Li, and Yu-Chiang Frank Wang. Dualmodality seq2seq network for audio-visual event localization. In ICASSP , 2019.</list_item>
<list_item><loc_259><loc_200><loc_452><loc_226>Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Ola: Pushing the frontiers of omni-modal language model. arXiv preprint arXiv:2502.04328 , 2025.</list_item>
<list_item><loc_259><loc_229><loc_452><loc_262>Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. In CVPR , 2024.</list_item>
<list_item><loc_259><loc_265><loc_452><loc_291>Jie Ma, Min Hu, Pinghui Wang, Wangchun Sun, Lingyun Song, Hongbin Pei, Jun Liu, and Youtian Du. Look, listen, and answer: Overcoming biases for audio-visual question answering. NeurIPS , 2024.</list_item>
<list_item><loc_259><loc_293><loc_452><loc_319>MuhammadMaaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424 , 2023.</list_item>
<list_item><loc_259><loc_322><loc_452><loc_334>Shentong Mo and Pedro Morgado. Localizing visual sounds the easy way. In ECCV , 2022.</list_item>
<list_item><loc_259><loc_337><loc_452><loc_356>Shentong Mo and Pedro Morgado. Unveiling the power of audio-visual early fusion transformers with dense interactions through masked modeling. In CVPR , 2024.</list_item>
<list_item><loc_259><loc_359><loc_452><loc_378>Liliane Momeni, Triantafyllos Afouras, Themos Stafylakis, Samuel Albanie, and Andrew Zisserman. Seeing wake words: Audio-visual keyword spotting. In BMVC , 2020.</list_item>
<list_item><loc_259><loc_381><loc_452><loc_400>Arsha Nagrani, Joon Son Chung, Samuel Albanie, and Andrew Zisserman. Disentangled speech embeddings using cross-modal self-supervision. In ICASSP , 2020.</list_item>
<list_item><loc_259><loc_402><loc_452><loc_421>Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia Schmid, and Chen Sun. Attention bottlenecks for multimodal fusion. In NeurIPS , 2021.</list_item>
<list_item><loc_259><loc_424><loc_452><loc_450>Medhini Narasimhan, Shiry Ginosar, Andrew Owens, Alexei A Efros, and Trevor Darrell. Strumming to the beat: Audio-conditioned contrastive video textures. arXiv preprint arXiv:2104.02687 , 2021.</list_item>
</unordered_list>
<page_footer><loc_243><loc_474><loc_257><loc_479>1036</page_footer>
<page_break>
<ordered_list><list_item><loc_48><loc_48><loc_241><loc_67>Andrew Owens and Alexei A Efros. Audio-visual scene analysis with self-supervised multisensory features. In ECCV , 2018.</list_item>
<list_item><loc_48><loc_69><loc_241><loc_88>Andrew Owens, Jiajun Wu, Josh H McDermott, William T Freeman, and Antonio Torralba. Ambient sound provides supervision for visual learning. In ECCV , 2016.</list_item>
<list_item><loc_48><loc_90><loc_241><loc_116>Andrew Owens, Jiajun Wu, Josh H McDermott, William T Freeman, and Antonio Torralba. Learning sight from sound: Ambient sound provides supervision for visual learning. IJCV , 2018.</list_item>
<list_item><loc_48><loc_117><loc_241><loc_144>Mandela Patrick, Yuki M Asano, Ruth Fong, Jo˜ ao F Henriques, Geoffrey Zweig, and Andrea Vedaldi. Multi-modal self-supervision from generalized data transformations. In NeurIPS , 2020.</list_item>
<list_item><loc_48><loc_145><loc_241><loc_164>KRPrajwal, Liliane Momeni, Triantafyllos Afouras, and Andrew Zisserman. Visual keyword spotting with attention. In BMVC , 2021.</list_item>
<list_item><loc_48><loc_166><loc_241><loc_185>Rui Qian, Di Hu, Heinrich Dinkel, Mengyue Wu, Ning Xu, and Weiyao Lin. Multiple sound sources localization from coarse to fine. In ECCV , 2020.</list_item>
<list_item><loc_48><loc_187><loc_241><loc_206>Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In ICLR , 2023.</list_item>
<list_item><loc_48><loc_208><loc_241><loc_227>Kun Su, Xiulong Liu, and Eli Shlizerman. Multiinstrumentalist net: Unsupervised generation of music from body movements. arXiv preprint arXiv:2012.03478 , 2020.</list_item>
<list_item><loc_48><loc_229><loc_241><loc_241>Kun Su, Xiulong Liu, and Eli Shlizerman. How does it sound? In NeurIPS , 2021.</list_item>
<list_item><loc_48><loc_243><loc_241><loc_262>Kun Su, Xiulong Liu, and Eli Shlizerman. From vision to audio and beyond: A unified model for audio-visual representation and generation. In ICML , 2024.</list_item>
<list_item><loc_48><loc_264><loc_241><loc_283>Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all. arXiv preprint arXiv:2305.16355 , 2023.</list_item>
<list_item><loc_48><loc_285><loc_241><loc_318>Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 , 2023.</list_item>
<list_item><loc_48><loc_320><loc_241><loc_339>Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu. Audio-visual event localization in unconstrained videos. In ECCV , 2018.</list_item>
<list_item><loc_48><loc_340><loc_241><loc_367>Efthymios Tzinis, Scott Wisdom, Aren Jansen, Shawn Hershey, Tal Remez, Daniel PW Ellis, and John R Hershey. Into the wild with audioscope: Unsupervised audio-visual separation of on-screen sounds. In ICLR , 2021.</list_item>
<list_item><loc_48><loc_368><loc_241><loc_387>Ilpo Viertola, Vladimir Iashin, and Esa Rahtu. Temporally aligned audio for video with autoregression. In ICASSP , 2025.</list_item>
<list_item><loc_48><loc_389><loc_241><loc_408>Olivia Wiles, A. Sophia Koepke, and Andrew Zisserman. X2face: A network for controlling face generation using images, audio, and pose codes. In ECCV , 2018.</list_item>
<list_item><loc_48><loc_410><loc_241><loc_422>Yu Wu, Linchao Zhu, Yan Yan, and Yi Yang. Dual attention matching for audio-visual event localization. In CVPR , 2019.</list_item>
<list_item><loc_48><loc_424><loc_241><loc_450>Fanyi Xiao, Yong Jae Lee, Kristen Grauman, Jitendra Malik, and Christoph Feichtenhofer. Audiovisual slowfast networks for video recognition. arXiv preprint arXiv:2001.08740 , 2020.</list_item>
<list_item><loc_259><loc_48><loc_452><loc_67>Haoming Xu, Runhao Zeng, Qingyao Wu, Mingkui Tan, and Chuang Gan. Cross-modal relation-aware networks for audio-visual event localization. In ACM MM , 2020.</list_item>
<list_item><loc_259><loc_69><loc_452><loc_88>Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, and Dahua Lin. Pointllm: Empowering large language models to understand point clouds. In ECCV , 2024.</list_item>
<list_item><loc_259><loc_91><loc_452><loc_117>An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388 , 2025.</list_item>
<list_item><loc_259><loc_119><loc_452><loc_138>Pinci Yang, Xin Wang, Xuguang Duan, Hong Chen, Runze Hou, Cong Jin, and Wenwu Zhu. Avqa: A dataset for audiovisual question answering on videos. In ACM MM , 2022.</list_item>
<list_item><loc_259><loc_140><loc_452><loc_159>Heeseung Yun, Youngjae Yu, Wonsuk Yang, Kangil Lee, and Gunhee Kim. Pano-avqa: Grounded audio-visual question answering on 360deg videos. In ICCV , 2021.</list_item>
<list_item><loc_259><loc_162><loc_452><loc_181>Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl Vondrick, Josh McDermott, and Antonio Torralba. The sound of pixels. In ECCV , 2018.</list_item>
<list_item><loc_259><loc_183><loc_452><loc_195>Hang Zhao, Chuang Gan, Wei-Chiu Ma, and Antonio Torralba. The sound of motions. In ICCV , 2019.</list_item>
<list_item><loc_259><loc_197><loc_452><loc_210>Hang Zhou, Ziwei Liu, Xudong Xu, Ping Luo, and Xiaogang Wang. Vision-infused deep audio inpainting. In ICCV , 2019.</list_item>
<list_item><loc_259><loc_212><loc_452><loc_231>Yipin Zhou, Zhaowen Wang, Chen Fang, Trung Bui, and Tamara L. Berg. Visual to sound: Generating natural sound for videos in the wild. In CVPR , 2018.</list_item>
<list_item><loc_259><loc_233><loc_452><loc_266>Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, et al. Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment. arXiv preprint arXiv:2310.01852 , 2023.</list_item>
<list_item><loc_259><loc_269><loc_452><loc_281>Lingyu Zhu and Esa Rahtu. V-slowfast network for efficient visual sound separation. In WACV , 2022.</list_item>
</ordered_list>
<page_footer><loc_243><loc_474><loc_257><loc_479>1037</page_footer>
</doctag>